import copy
import math
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score

# 读取xlsx文件
df = pd.read_excel('try.xlsx', engine='openpyxl')
print(df)

# 从DataFrame中提取x和y的值
X_train = df[['x', 'x2']].values
y_train = df['value'].values

initial_b = 0
initial_w = np.array([0, 0])
iterations = 1000
alpha = 0.0001
# 正则化参数
lambda_ = 1

def predict_single_loop(x, w, b):
    n = x.shape[0]
    p = 0
    for i in range(n):
        p_i = x[i] * w[i]
        p = p + p_i
    p = p + b
    return p

def predict(x, w, b):
    p = np.dot(x, w) + b
    return p

def compute_cost(X, y, w, b, lambda_):
    m = X.shape[0]
    n = len(w)
    cost = 0.
    for i in range(m):
        f_wb_i = np.dot(X[i], w) + b
        cost = cost + (f_wb_i - y[i]) ** 2
    cost = cost / (2 * m)

    reg_cost = 0
    for j in range(n):
        reg_cost += (w[j] ** 2)
    reg_cost = (lambda_ / (2 * m)) * reg_cost

    total_cost = cost + reg_cost
    return total_cost

def compute_gradient(X, y, w, b):
    m, n = X.shape  # (number of examples, number of features)
    dj_dw = np.zeros((n,))
    dj_db = 0.

    for i in range(m):
        err = (np.dot(X[i], w) + b) - y[i]
        for j in range(n):
            dj_dw[j] = dj_dw[j] + err * X[i, j]
        dj_db = dj_db + err
    dj_dw = dj_dw / m
    dj_db = dj_db / m

    for j in range(n):
        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]

    return dj_db, dj_dw

def gradient_descent(X, y, w_in, b_in, compute_cost, compute_gradient, alpha, num_iters, lambda_):
    J_history = []
    w = copy.deepcopy(w_in)
    b = b_in

    for i in range(num_iters):
        dj_db, dj_dw = compute_gradient(X, y, w, b)
        w = w - alpha * dj_dw
        b = b - alpha * dj_db

        # Save cost J at each iteration
        if i < 100000:  # prevent resource exhaustion
            J_history.append(compute_cost(X, y, w, b, lambda_))

        # Print cost every at intervals 10 times or as many iterations if < 10
        if i % math.ceil(num_iters / 10) == 0:
            print(f"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   ")

    return w, b, J_history  # return final w,b and J history for graphing

w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,
                                                    compute_cost, compute_gradient,
                                                    alpha, iterations, lambda_)

# 绘制成本历史
plt.plot(J_hist)
plt.xlabel('iterations')
plt.ylabel('cost')
plt.title('cost_history')

# 显示图表
plt.show()

print(f"b found by gradient descent: {b_final:0.2f} ")
print(f"w found by gradient descent: {w_final} ")

y_pre = np.dot(X_train, w_final) + b_final
# 计算 R²
r2 = r2_score(y_train, y_pre)
print("R²:", r2)

# 计算 MSE
mse = mean_squared_error(y_train, y_pre)
print("均方误差MSE:", mse)

# 计算 RMSE
rmse = np.sqrt(mse)
print("均方根误差RMSE:", rmse)
