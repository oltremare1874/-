import pandas as pd
import numpy as np
from scipy.stats import kurtosis
from pmdarima import auto_arima
import pmdarima as pm
import torch
import torch.nn as nn
from sklearn.metrics import mean_squared_error
import json
from sklearn.preprocessing import MinMaxScaler
import pickle
import matplotlib
matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
import math
import matplotlib.dates as mdates

def mean_absolute_percentage_error(actual, prediction):
    actual = pd.Series(actual)
    prediction = pd.Series(prediction)
    return 100 * np.mean(np.abs((actual - prediction))/actual)

def get_arima(data, train_len, test_len, forecast_len):
    data = data.tail(test_len + train_len).reset_index(drop=True)
    train = data.head(train_len).values.tolist()
    test = data.tail(test_len).values.tolist()

    # Initialize model
    model = auto_arima(train, max_p=10, max_q=10, seasonal=True, trace=True,stepwise=False,
                       error_action='ignore', suppress_warnings=True, maxiter=50, method='nm')

    # Determine model parameters
    model.fit(train)
    order = model.get_params()['order']
    print('ARIMA order:', order, '\n')

    # Genereate predictions
    prediction = []
    for i in range(len(test)):
        model = pm.ARIMA(order=order)
        model.fit(train)
        print('working on', i + 1, 'of', test_len, '-- ' + str(int(100 * (i + 1) / test_len)) + '% complete')
        prediction.append(model.predict()[0])
        train.append(test[i])

    # Generate error data
    mse = mean_squared_error(test, prediction)
    rmse = mse ** 0.5
    mape = mean_absolute_percentage_error(pd.Series(test), pd.Series(prediction))

    forecast_train = model.predict_in_sample()

    forecast = model.predict(n_periods=forecast_len)

    # 保存ARIMA模型
    with open('model_arima.pkl', 'wb') as pkl:
        pickle.dump(model, pkl)
    return prediction, mse, rmse, mape, forecast_train, forecast


class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim=1, num_layers=2):
        super(LSTMModel, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        # Define the LSTM layer
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)

        # Define the output layer
        self.linear = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        # Initialize hidden state with zeros
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()

        # Initialize cell state
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()

        # We need to detach as we are doing truncated backpropagation through time (BPTT)
        # If we don't, we'll backprop all the way to the start even after going through another batch
        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))

        # Index hidden state of last time step
        out = self.linear(out[:, -1, :])
        return out

def predict_future(model, last_data, steps, lstm_len, scaler):
    model.eval()  # 将模型设置为评估模式
    predictions = []
    current_input = last_data[-lstm_len:]  # 获取最后 lstm_len 个数据点

    for _ in range(steps):
        x = torch.tensor(current_input).float()
        x = x.view(1, lstm_len, -1)  # 重塑为正确的输入形状

        # 预测下一个点
        with torch.no_grad():
            pred = model(x)
        pred_value = pred.cpu().data.numpy()[0, 0]  # 获取预测值
        predictions.append(pred_value)

        # 更新输入数据
        current_input.pop(0)  # 移除最早的数据点
        current_input.append(pred_value)  # 添加预测的数据点

    predictions = np.array(predictions)
    predictions = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()
    return predictions

def get_lstm(data, train_len, test_len, lstm_len, forecast_len):
    # prepare train and test data
    data = data.tail(test_len + train_len).reset_index(drop=True)
    dataset = np.reshape(data.values, (len(data), 1))
    scaler = MinMaxScaler(feature_range=(0, 1))
    dataset_scaled = scaler.fit_transform(dataset)
    x_train = []
    y_train = []
    x_test = []

    for i in range(lstm_len, train_len):
        x_train.append(dataset_scaled[i - lstm_len:i, 0])
        y_train.append(dataset_scaled[i, 0])
    for i in range(train_len, len(dataset_scaled)):
        x_test.append(dataset_scaled[i - lstm_len:i, 0])

    x_train = np.array(x_train)
    y_train = np.array(y_train)
    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
    x_test = np.array(x_test)
    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))

    # Convert to PyTorch tensors
    x_train = torch.tensor(x_train).float()
    y_train = torch.tensor(y_train).float()
    x_test = torch.tensor(x_test).float()

    # Create the PyTorch model
    model = LSTMModel(input_dim=1, hidden_dim=lstm_len)
    criterion = torch.nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

    total_loss = 0
    # train
    for epoch in range(500):
        model.train()
        optimizer.zero_grad()

        # Forward pass
        y_pred = model(x_train)

        # Compute Loss
        loss = criterion(y_pred.squeeze(), y_train)
        total_loss += loss.item()

        # Backward pass and optimize
        loss.backward()
        optimizer.step()

        # Print training progress
        if (epoch + 1) % 50 == 0:  # 每50轮打印一次
            print(f'Epoch [{epoch + 1}/500], Loss: {loss.item():.4f}')

    # Calculate and print average loss
    average_loss = total_loss / 500
    print(f'Average Loss: {average_loss:.4f}')
    # Prediction
    model.eval()
    predict = model(x_test)
    predict = predict.data.numpy()
    prediction = scaler.inverse_transform(predict).tolist()

    output = []
    for i in range(len(prediction)):
        output.extend(prediction[i])
    prediction = output

    # Error calculation
    mse = mean_squared_error(data.tail(len(prediction)).values, prediction)
    rmse = mse ** 0.5
    mape = mean_absolute_percentage_error(data.tail(len(prediction)).reset_index(drop=True), pd.Series(prediction))

    forecast_train = model(x_train)
    forecast_train = forecast_train.data.numpy()
    forecast_train = scaler.inverse_transform(forecast_train).tolist()
    output_ = []
    for j in range(len(forecast_train)):
        output_.extend(forecast_train[j])
    forecast_train = output_

    # 预测
    future_predictions = predict_future(model, data.tolist(), forecast_len, lstm_len, scaler)

    # 保存LSTM模型
    torch.save(model.state_dict(), 'model_lstm.pth')
    return prediction, mse, rmse, mape, scaler, forecast_train, future_predictions


def SMA(data, window):
    sma = np.convolve(data[target], np.ones(window), 'same') / window
    return sma


def EMA(data, window):
    alpha = 2 / (window + 1)
    ema = np.zeros_like(data)
    ema[0] = data.iloc[0]  # 设置初始值为序列的第一个值

    for i in range(1, len(data)):
        ema[i] = alpha * data.iloc[i] + (1 - alpha) * ema[i - 1]

    return ema


def WMA(data, window):
    weights = np.arange(1, window + 1)
    wma = np.convolve(data[target], weights / weights.sum(), 'same')
    return wma


# 其他复杂的移动平均技术如 DEMA 可以通过组合上述基础方法实现
# 例如，DEMA 是两个不同窗口大小的 EMA 的组合

if __name__ == '__main__':
    # Load historical data from Excel
    # Excel should have columns: ['Date', 'Value']
    excel_file_path = 'btb.xlsx'
    target = 'Value'  # Change this to the column name you want to predict
    data = pd.read_excel(excel_file_path, index_col='Date', parse_dates=True).reset_index(drop=True)[[target]]
    ddata = pd.read_excel('btb.xlsx', engine='openpyxl')
    ddata['Date'] = pd.to_datetime(ddata['Date'])
    ddata = ddata.set_index('Date')
    look_back = 5
    forecast_len = 30
    talib_moving_averages = ['SMA']  # 替换你想用的方法

    # 创建一个字典来存储这些函数
    functions = {
         'SMA': SMA,
         'EMA': EMA,
         'WMA': WMA,
        # 添加其他需要的移动平均函数
    }

    # for ma in talib_moving_averages:
    #     functions[ma] = abstract.Function(ma)
    train_len = math.floor(len(data) * 0.7)
    test_len = len(data) - train_len
    # Determine kurtosis "K" values for MA period 4-99
    kurtosis_results = {'period': []}
    for i in range(4, 100):
        kurtosis_results['period'].append(i)
        for ma in talib_moving_averages:
            ma_output = functions[ma](data[:-test_len], i)[-60:]
            # Determine kurtosis "K" value
            k = kurtosis(ma_output, fisher=False)

            # add to dictionary
            if ma not in kurtosis_results.keys():
                kurtosis_results[ma] = []
            kurtosis_results[ma].append(k)

    kurtosis_results = pd.DataFrame(kurtosis_results)
    #   kurtosis_results.to_csv('kurtosis_results.csv')

    # Determine period with K closest to 3 +/-5%
    optimized_period = {}
    for ma in talib_moving_averages:
        difference = np.abs(kurtosis_results[ma] - 3)
        df = pd.DataFrame({'difference': difference, 'period': kurtosis_results['period']})
        df = df.sort_values(by=['difference'], ascending=True).reset_index(drop=True)
        if df.at[0, 'difference'] < 3 * 0.05:
            optimized_period[ma] = df.at[0, 'period']
        else:
            print(ma + ' is not viable, best K greater or less than 3 +/-5%')

    print('\nOptimized periods:', optimized_period)

    simulation = {}
    for ma in optimized_period:
        # Split data into low volatility and high volatility time series
        low_vol = pd.Series(functions[ma](data, optimized_period[ma]))
        high_vol = pd.Series(data[target] - low_vol)

        # Generate ARIMA and LSTM predictions
        print('\nWorking on ' + ma + ' predictions')
        try:
            low_vol_prediction, low_vol_mse, low_vol_rmse, low_vol_mape, train_forecast_arima, arima_forecast = get_arima(low_vol, train_len, test_len, forecast_len)
        except:
            print('ARIMA error, skipping to next MA type')
            continue

        high_vol_prediction, high_vol_mse, high_vol_rmse, high_vol_mape, scaler, train_forecast_lstm, lstm_forecast = get_lstm(high_vol, train_len, test_len, look_back, forecast_len)

        test_prediction = pd.Series(low_vol_prediction) + pd.Series(high_vol_prediction)
        mse = mean_squared_error(test_prediction.values, data[target].tail(test_len).values)
        rmse = mse ** 0.5
        mape = mean_absolute_percentage_error(data[target].tail(test_len).reset_index(drop=True), test_prediction)

        # Generate prediction accuracy
        text_actual = data[target].tail(test_len).values
        df = pd.DataFrame({'real': text_actual, 'pre': test_prediction}).to_csv('results.csv', index=False)
        result_1 = []
        result_2 = []
        for i in range(1, len(test_prediction)):
            # Compare prediction to previous close price
            if test_prediction[i] > text_actual[i - 1] and text_actual[i] > text_actual[i - 1]:
                result_1.append(1)
            elif test_prediction[i] < text_actual[i - 1] and text_actual[i] < text_actual[i - 1]:
                result_1.append(1)
            else:
                result_1.append(0)

            # Compare prediction to previous prediction
            if test_prediction[i] > test_prediction[i - 1] and text_actual[i] > text_actual[i - 1]:
                result_2.append(1)
            elif test_prediction[i] < test_prediction[i - 1] and text_actual[i] < text_actual[i - 1]:
                result_2.append(1)
            else:
                result_2.append(0)

        accuracy_1 = np.mean(result_1)
        accuracy_2 = np.mean(result_2)

        simulation[ma] = {'low_vol': {'prediction': low_vol_prediction, 'mse': low_vol_mse,
                                      'rmse': low_vol_rmse, 'mape': low_vol_mape},
                          'high_vol': {'prediction': high_vol_prediction, 'mse': high_vol_mse,
                                       'rmse': high_vol_rmse},
                          'final': {'prediction': test_prediction.values.tolist(), 'mse': mse,
                                    'rmse': rmse, 'mape': mape},
                          'accuracy': {'prediction vs close': accuracy_1, 'prediction vs prediction': accuracy_2}}

        # save simulation data here as checkpoint
        with open('simulation_data.json', 'w') as fp:
            json.dump(simulation, fp)

        train_predict = pd.Series(train_forecast_arima) + pd.Series(train_forecast_lstm)
        train_predict = train_predict.dropna()
        # 实际值与预测值的索引
        train_predict_index = range(look_back, len(train_predict) + look_back)
        test_predict_index = range(len(train_predict) + (1 * look_back), len(data))
        # 绘制训练集预测结果
        plt.figure(figsize=(8, 6))
        plt.plot(ddata.index, ddata[target].values, label='Actual Data', color='#318AE3')
        plt.plot(ddata.index[list(train_predict_index)], train_predict, label='Train Predict', color='#E68300')
        plt.plot(ddata.index[list(test_predict_index)], test_prediction, label='Test Predict', color='#FF1E0D')
        plt.title('Train and Test Predictions vs Actual')
        plt.xlabel('Date')
        plt.ylabel('Number of Reported Results')
        plt.legend()
        plt.show()



        # 绘制测试集预测结果
        # 创建图表和主坐标轴
        fig, ax1 = plt.subplots(figsize=(10, 50))
        # 绘制实际值和预测值
        ax1.plot(ddata.index[list(test_predict_index)], text_actual, label='Actual Test Data', color='#318AE3')
        ax1.plot(ddata.index[list(test_predict_index)], test_prediction, label='Predicted Test Data', color='#ED6969')
        ax1.set_title('Test Set Prediction with Error Bars')
        ax1.set_xlabel('Date')
        ax1.set_ylabel('Number of Reported Results')
        ax1.legend(loc='upper left')
        # 计算误差的绝对值
        errors = np.abs(text_actual - test_prediction)
        # 创建次坐标轴
        ax2 = ax1.twinx()
        # 绘制面积图
        ax2.fill_between(ddata.index[list(test_predict_index)], 0, errors, label='Prediction Error', color='#39B036',
                         alpha=0.5, edgecolor='#299729', linewidth=1.0)
        # 设置y轴标签
        ax2.set_ylabel('Error')
        # 设置主坐标轴的y轴范围
        ax1.set_ylim(10000, 70000)
        # 设置次坐标轴的y轴范围
        ax2.set_ylim(0, 7000)
        # 添加图例
        ax2.legend(loc='upper right')
        # 设置x轴的日期格式
        ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
        ax1.xaxis.set_major_locator(mdates.DayLocator(interval=60))
        plt.xticks(rotation=45)
        # 显示图表
        plt.show()


        # 生成未来日期
        last_date = ddata.index[-1]
        future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_len)
        predicted_values = pd.Series(arima_forecast) + pd.Series(lstm_forecast)
        # 绘制图表
        plt.figure(figsize=(12, 6))
        plt.plot(ddata.index, data['Value'], label='Original Data', color='#318AE3')
        plt.plot(future_dates, predicted_values, label='Predicted Data', color='#FF1E0D')
        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
        plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=30))
        plt.xticks(rotation=45)
        plt.legend()
        plt.xlabel('Date')
        plt.ylabel('Number of Reported Results')
        plt.title('Time Series Forecasting')
        plt.show()

        # 创建一个包含日期和预测值的 DataFrame
        predictions_df = pd.DataFrame({
            'Date': future_dates,
            'Predicted Values': predicted_values
        })
        print(predictions_df)

    for ma in simulation.keys():
        print('\n' + ma)
        print('Prediction vs Close:\t\t' + str(round(100 * simulation[ma]['accuracy']['prediction vs close'], 2))
              + '% Accuracy')
        print(
            'Prediction vs Prediction:\t' + str(round(100 * simulation[ma]['accuracy']['prediction vs prediction'], 2))
            + '% Accuracy')
        print('MSE:\t', simulation[ma]['final']['mse'],
              '\nRMSE:\t', simulation[ma]['final']['rmse'],
              '\nMAPE:\t', simulation[ma]['final']['mape'])
